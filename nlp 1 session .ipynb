{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64262cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Vigneswaran\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "811d959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"Hi everyone this  is vignesh from chennai and ia so happy to be here. iam learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "994c7d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c03cc8",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b03efd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f34dd7ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'everyone',\n",
       " 'this',\n",
       " 'is',\n",
       " 'vignesh',\n",
       " 'from',\n",
       " 'chennai',\n",
       " 'and',\n",
       " 'ia',\n",
       " 'so',\n",
       " 'happy',\n",
       " 'to',\n",
       " 'be',\n",
       " 'here',\n",
       " '.',\n",
       " 'iam',\n",
       " 'learning']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0102a6",
   "metadata": {},
   "source": [
    "## stemmer\n",
    "In stemming u dont find a proper meaning for a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a19f1a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "12abbfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f632264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'organ'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('organization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "efe6d1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'histori'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('history')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff90cd81",
   "metadata": {},
   "source": [
    "## Lemmatizer\n",
    "To overcome the issue of improper meaning word  in stemmer we use lemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2878229b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Vigneswaran\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5e1e6475",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2a33ea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "66b3b6ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'history'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "94513482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vicky'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"vicky\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2c4efbc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi everyone this  is vignesh from chennai and ia so happy to be here.',\n",
       " 'iam learning']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "29f67539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'everyon', 'thi', 'is', 'vignesh', 'from', 'chennai', 'and', 'ia', 'so', 'happi', 'to', 'be', 'here', '.']\n",
      "['iam', 'learn']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentence)):\n",
    "    words = nltk.word_tokenize(sentence[i])\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8b251a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'everyone', 'this', 'is', 'vignesh', 'from', 'chennai', 'and', 'ia', 'so', 'happy', 'to', 'be', 'here', '.']\n",
      "['iam', 'learning']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentence)):\n",
    "    words = nltk.word_tokenize(sentence[i])\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b461f9",
   "metadata": {},
   "source": [
    "## StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cfaf30fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Vigneswaran\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "feb77d80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "42a69678",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "27004b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mj = \"\"\"\n",
    "\n",
    "Michael Joseph Jackson (August 29, 1958 – June 25, 2009) was an American singer, songwriter, dancer, and philanthropist. Dubbed the \"King of Pop\", he is regarded as one of the most significant cultural figures of the 20th century. Over a four-decade career, his contributions to music, dance, and fashion, along with his publicized personal life, made him a global figure in popular culture. Jackson influenced artists across many music genres. Through stage and video performances, he popularized street dance moves such as the moonwalk, which he named, and the robot.He was the eighth child of the Jackson family, and made his public debut in 1964 with his older brothers Jackie, Tito, Jermaine, and Marlon as a member of the Jackson 5 (later known as the Jacksons). The Jackson 5 signed with Motown in 1968 and achieved worldwide success with Michael as lead singer. Jackson began his solo career in 1971 while at Motown and recorded multiple successful singles. He became a global solo star with his 1979 album Off the Wall. His music videos, including those for \"Beat It\", \"Billie Jean\", and \"Thriller\" from his 1982 album Thriller, are credited with breaking racial barriers and transforming the medium into an art form and promotional tool. He helped popularize MTV and continued to innovate with videos for his albums Bad (1987), Dangerous (1991), HIStory: Past, Present and Future, Book I (1995), and Invincible (2001)'\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f7818c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nMichael Joseph Jackson (August 29, 1958 – June 25, 2009) was an American singer, songwriter, dancer, and philanthropist. Dubbed the \"King of Pop\", he is regarded as one of the most significant cultural figures of the 20th century. Over a four-decade career, his contributions to music, dance, and fashion, along with his publicized personal life, made him a global figure in popular culture. Jackson influenced artists across many music genres. Through stage and video performances, he popularized street dance moves such as the moonwalk, which he named, and the robot.He was the eighth child of the Jackson family, and made his public debut in 1964 with his older brothers Jackie, Tito, Jermaine, and Marlon as a member of the Jackson 5 (later known as the Jacksons). The Jackson 5 signed with Motown in 1968 and achieved worldwide success with Michael as lead singer. Jackson began his solo career in 1971 while at Motown and recorded multiple successful singles. He became a global solo star with his 1979 album Off the Wall. His music videos, including those for \"Beat It\", \"Billie Jean\", and \"Thriller\" from his 1982 album Thriller, are credited with breaking racial barriers and transforming the medium into an art form and promotional tool. He helped popularize MTV and continued to innovate with videos for his albums Bad (1987), Dangerous (1991), HIStory: Past, Present and Future, Book I (1995), and Invincible (2001)\\'\\n\\n'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4d715068",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(mj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "40f4380d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Michael', 'Joseph', 'Jackson', '(', 'August', '29', ',', '1958', '–', 'June', '25', ',', '2009', ')', 'wa', 'an', 'American', 'singer', ',', 'songwriter', ',', 'dancer', ',', 'and', 'philanthropist', '.']\n",
      "['Dubbed', 'the', '``', 'King', 'of', 'Pop', \"''\", ',', 'he', 'is', 'regarded', 'a', 'one', 'of', 'the', 'most', 'significant', 'cultural', 'figure', 'of', 'the', '20th', 'century', '.']\n",
      "['Over', 'a', 'four-decade', 'career', ',', 'his', 'contribution', 'to', 'music', ',', 'dance', ',', 'and', 'fashion', ',', 'along', 'with', 'his', 'publicized', 'personal', 'life', ',', 'made', 'him', 'a', 'global', 'figure', 'in', 'popular', 'culture', '.']\n",
      "['Jackson', 'influenced', 'artist', 'across', 'many', 'music', 'genre', '.']\n",
      "['Through', 'stage', 'and', 'video', 'performance', ',', 'he', 'popularized', 'street', 'dance', 'move', 'such', 'a', 'the', 'moonwalk', ',', 'which', 'he', 'named', ',', 'and', 'the', 'robot.He', 'wa', 'the', 'eighth', 'child', 'of', 'the', 'Jackson', 'family', ',', 'and', 'made', 'his', 'public', 'debut', 'in', '1964', 'with', 'his', 'older', 'brother', 'Jackie', ',', 'Tito', ',', 'Jermaine', ',', 'and', 'Marlon', 'a', 'a', 'member', 'of', 'the', 'Jackson', '5', '(', 'later', 'known', 'a', 'the', 'Jacksons', ')', '.']\n",
      "['The', 'Jackson', '5', 'signed', 'with', 'Motown', 'in', '1968', 'and', 'achieved', 'worldwide', 'success', 'with', 'Michael', 'a', 'lead', 'singer', '.']\n",
      "['Jackson', 'began', 'his', 'solo', 'career', 'in', '1971', 'while', 'at', 'Motown', 'and', 'recorded', 'multiple', 'successful', 'single', '.']\n",
      "['He', 'became', 'a', 'global', 'solo', 'star', 'with', 'his', '1979', 'album', 'Off', 'the', 'Wall', '.']\n",
      "['His', 'music', 'video', ',', 'including', 'those', 'for', '``', 'Beat', 'It', \"''\", ',', '``', 'Billie', 'Jean', \"''\", ',', 'and', '``', 'Thriller', \"''\", 'from', 'his', '1982', 'album', 'Thriller', ',', 'are', 'credited', 'with', 'breaking', 'racial', 'barrier', 'and', 'transforming', 'the', 'medium', 'into', 'an', 'art', 'form', 'and', 'promotional', 'tool', '.']\n",
      "['He', 'helped', 'popularize', 'MTV', 'and', 'continued', 'to', 'innovate', 'with', 'video', 'for', 'his', 'album', 'Bad', '(', '1987', ')', ',', 'Dangerous', '(', '1991', ')', ',', 'HIStory', ':', 'Past', ',', 'Present', 'and', 'Future', ',', 'Book', 'I', '(', '1995', ')', ',', 'and', 'Invincible', '(', '2001', ')', \"'\"]\n"
     ]
    }
   ],
   "source": [
    "corpus=[]\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    print(words)\n",
    "    corpus.append(' '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8945a10f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Michael Joseph Jackson ( August 29 , 1958 – June 25 , 2009 ) wa an American singer , songwriter , dancer , and philanthropist .',\n",
       " \"Dubbed the `` King of Pop '' , he is regarded a one of the most significant cultural figure of the 20th century .\",\n",
       " 'Over a four-decade career , his contribution to music , dance , and fashion , along with his publicized personal life , made him a global figure in popular culture .',\n",
       " 'Jackson influenced artist across many music genre .',\n",
       " 'Through stage and video performance , he popularized street dance move such a the moonwalk , which he named , and the robot.He wa the eighth child of the Jackson family , and made his public debut in 1964 with his older brother Jackie , Tito , Jermaine , and Marlon a a member of the Jackson 5 ( later known a the Jacksons ) .',\n",
       " 'The Jackson 5 signed with Motown in 1968 and achieved worldwide success with Michael a lead singer .',\n",
       " 'Jackson began his solo career in 1971 while at Motown and recorded multiple successful single .',\n",
       " 'He became a global solo star with his 1979 album Off the Wall .',\n",
       " \"His music video , including those for `` Beat It '' , `` Billie Jean '' , and `` Thriller '' from his 1982 album Thriller , are credited with breaking racial barrier and transforming the medium into an art form and promotional tool .\",\n",
       " \"He helped popularize MTV and continued to innovate with video for his album Bad ( 1987 ) , Dangerous ( 1991 ) , HIStory : Past , Present and Future , Book I ( 1995 ) , and Invincible ( 2001 ) '\"]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05faa549",
   "metadata": {},
   "source": [
    "## using stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "86e08f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['michael', 'joseph', 'jackson', '(', 'august', '29', ',', '1958', '–', 'june', '25', ',', '2009', ')', 'american', 'singer', ',', 'songwrit', ',', 'dancer', ',', 'philanthropist', '.']\n",
      "['dub', '``', 'king', 'pop', \"''\", ',', 'regard', 'one', 'signific', 'cultur', 'figur', '20th', 'centuri', '.']\n",
      "['over', 'four-decad', 'career', ',', 'contribut', 'music', ',', 'danc', ',', 'fashion', ',', 'along', 'public', 'person', 'life', ',', 'made', 'global', 'figur', 'popular', 'cultur', '.']\n",
      "['jackson', 'influenc', 'artist', 'across', 'mani', 'music', 'genr', '.']\n",
      "['through', 'stage', 'video', 'perform', ',', 'popular', 'street', 'danc', 'move', 'moonwalk', ',', 'name', ',', 'robot.h', 'eighth', 'child', 'jackson', 'famili', ',', 'made', 'public', 'debut', '1964', 'older', 'brother', 'jacki', ',', 'tito', ',', 'jermain', ',', 'marlon', 'member', 'jackson', '5', '(', 'later', 'known', 'jackson', ')', '.']\n",
      "['the', 'jackson', '5', 'sign', 'motown', '1968', 'achiev', 'worldwid', 'success', 'michael', 'lead', 'singer', '.']\n",
      "['jackson', 'began', 'solo', 'career', '1971', 'motown', 'record', 'multipl', 'success', 'singl', '.']\n",
      "['he', 'becam', 'global', 'solo', 'star', '1979', 'album', 'off', 'wall', '.']\n",
      "['hi', 'music', 'video', ',', 'includ', '``', 'beat', 'it', \"''\", ',', '``', 'billi', 'jean', \"''\", ',', '``', 'thriller', \"''\", '1982', 'album', 'thriller', ',', 'credit', 'break', 'racial', 'barrier', 'transform', 'medium', 'art', 'form', 'promot', 'tool', '.']\n",
      "['he', 'help', 'popular', 'mtv', 'continu', 'innov', 'video', 'album', 'bad', '(', '1987', ')', ',', 'danger', '(', '1991', ')', ',', 'histori', ':', 'past', ',', 'present', 'futur', ',', 'book', 'i', '(', '1995', ')', ',', 'invinc', '(', '2001', ')', \"'\"]\n"
     ]
    }
   ],
   "source": [
    "corpus=[]\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "     f\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [stemmer.stem(word) for word in words if not word in stop_words]\n",
    "    print(words)\n",
    "    corpus.append(' '.join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8557373",
   "metadata": {},
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0d4c1591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\vigneswaran\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\vigneswaran\\anaconda3\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\vigneswaran\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\vigneswaran\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\vigneswaran\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "677d9253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Vigneswaran\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Ensure necessary resources are downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Read the file\n",
    "with open('nlp1.txt', 'r') as file:\n",
    "    text = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "84855b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "782bd98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names: ['ability' 'advances' 'analysis' 'analyze' 'and' 'applications' 'as'\n",
      " 'assistants' 'automating' 'been' 'bert' 'by' 'can' 'chatbots' 'combines'\n",
      " 'computer' 'continually' 'data' 'deep' 'derive' 'developments' 'driven'\n",
      " 'enable' 'enhancing' 'entity' 'fascinating' 'field' 'from' 'generate'\n",
      " 'gpt' 'have' 'human' 'improve' 'in' 'interactions' 'interpret' 'is'\n",
      " 'language' 'learning' 'like' 'linguistics' 'machine' 'machines' 'meaning'\n",
      " 'models' 'named' 'natural' 'nlp' 'of' 'part' 'particularly' 'process'\n",
      " 'processing' 'range' 'recognition' 'science' 'sentiment' 'speech' 'such'\n",
      " 'systems' 'tagging' 'tasks' 'techniques' 'technologies' 'text' 'that'\n",
      " 'the' 'these' 'through' 'to' 'tokenization' 'translation' 'understand'\n",
      " 'user' 'various' 'virtual' 'with']\n",
      "Bag of Words Matrix:\n",
      " [[0 0 0 0 2 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 1\n",
      "  1 2 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 2 0 0\n",
      "  1 0 0 0 0]\n",
      " [0 0 0 1 2 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0\n",
      "  0 0 1 0 0]\n",
      " [0 0 1 0 2 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      "  0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1\n",
      "  0 0 0 1 0]\n",
      " [0 1 0 0 2 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 1 0 0 2 0 0\n",
      "  0 0 2 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1]\n",
      " [1 0 0 0 2 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0\n",
      "  0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 1 0 0\n",
      "  1 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert the result to an array and get feature names\n",
    "X_array = X.toarray()\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the feature names and the BoW representation\n",
    "print(\"Feature Names:\", feature_names)\n",
    "print(\"Bag of Words Matrix:\\n\", X_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a3ed77",
   "metadata": {},
   "source": [
    "## Stemmer implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd149344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e7c7534",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Vigneswaran\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Vigneswaran\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5168cfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8516b8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b94dee3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file\n",
    "with open('nlp1.txt', 'r') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e7a3f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ba28624",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "# Apply stemming\n",
    "stemmed_words = [stemmer.stem(word) for word in filtered_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f427c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natur languag process ( nlp ) fascin field combin comput scienc linguist enabl machin understand , interpret , gener human languag . variou techniqu token , part-of-speech tag , name entiti recognit , nlp model analyz deriv mean text data . applic rang chatbot virtual assist sentiment analysi languag translat . advanc nlp driven develop machin learn deep learn , particularli model like bert gpt . technolog continu improv abil system process understand natur languag , enhanc user interact autom task .\n"
     ]
    }
   ],
   "source": [
    "stemmed_text = ' '.join(stemmed_words)\n",
    "\n",
    "print(stemmed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce747df",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "825d0693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9452c0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Vigneswaran\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Vigneswaran\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Vigneswaran\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2413752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Load stop words\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3671d49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nlp1.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Tokenize the text\n",
    "words = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6bbaba30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Processing ( NLP ) fascinating field combine computer science linguistics enable machine understand , interpret , generate human language . various technique tokenization , part-of-speech tagging , named entity recognition , NLP model analyze derive meaning text data . Applications range chatbots virtual assistant sentiment analysis language translation . Advances NLP driven development machine learning deep learning , particularly model like BERT GPT . technology continually improve ability system process understand natural language , enhancing user interaction automating task .\n"
     ]
    }
   ],
   "source": [
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "# Apply lemmatization\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "\n",
    "# Join the lemmatized words back into a single string\n",
    "lemmatized_text = ' '.join(lemmatized_words)\n",
    "\n",
    "# Print the lemmatized text\n",
    "print(lemmatized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4c9e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e204e636",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "537c30d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ff1959",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3b1ada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c87f65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
